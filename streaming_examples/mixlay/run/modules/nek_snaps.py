#################################################################
# Read Nek5000 snapshot data and create databases from them
#  * Reading Nek5000 data is by `pymech`
#  * `pickle` is used to dump the database.
#################################################################
# Saleh Rezaeiravesh, salehr@kth.se
#----------------------------------------------------------------
# NOTE: In 2D flows, x and y assumed to be the active dimensions.
#
import numpy as np
from pymech import dataset
import pickle
from datetime import datetime
#
#
def dbCreator(info):
    """
    Create pickle database from Nek5000 data constructed over the whole mesh. 
    The database may contain the data of several quantities of interest (QoIs).
    The database of each QoI is created by `dbCreator_1QoI(...)`.

    The Nek5000 snapshot data are in `<caseName>0.f*` files.

    Args: 
       `info`: dict
          * `dataPath`: string, path at which Nek5000 `<caseName>0.f*` files are stored.
          * `caseName`: string, Nek5000 case name <caseName>.
          * `startID`: int, starting number of `<caseName>0.f*` to be included in the database.
          * `endID`: int, ending number of `<caseName>0.f*` to be included in the database.
          * `qoiName`: list, list of quantities of interest to be extracted from the Nek5000 data. 
          The valid QoIs include, 'ux', 'uy', 'uz', 'temperature, 'pressure'. Note that the inclusion 
          of the QoIs depends on the Nek5000 case and hence has to be specified by the user.

    Returns:
       `db`: dict, 
          * `qoiName`: list, list of QoIs whose data are included in the database. 
          For the keys of the database associated to each QoI, see `dbCreator_1QoI(...)`
          * `massMat`: numpy array of shape (nx,ny) containing the mass matrix of Nek5000 simulation            The created `db` is dumped as a `pickle`  dataset at `info['dataPath']`.
    """
    db={}

    path_=info['dataPath']
    caseName_=info['caseName']
    start_=info['startID']
    end_=info['endID']
    pickleFile_=path_+caseName_+'_'+str(start_)+'to'+str(end_)

    #read data of the QoIs
    for qoiName_ in info['qoiName']:
        print('... Reading data and creating db for ',qoiName_)
        db_=dbCreator_1QoI(info,qoiName_)
        db_.update({'sourceData':pickleFile_})
        db.update({qoiName_:db_})

    #read the mass matrix    
    dbm=massMatrixReader(info)
    dbm.update({'sourceData':pickleFile_})
    db.update({'massMat':dbm})

    #dump the pickle data    
    with open(pickleFile_,'wb') as F:
        pickle.dump(db,F)
    print('... pickle database created and saved at:')     
    print(pickleFile_)
    return db
#        
def dbCreator_1QoI(info,qoiName):
    """
    Create database for only one QoI, from Nek5000 data constructed over the whole mesh. 
    Reading `<caseName>0.f*` files generated by Nek5000 and create a database for the `qoiName`

    NOTE: In current implementation to read the Nek5000 data, method `dataset.open_dataset` from `pymech`
      is called. This requires the Nek5000 mesh to be Cartesian and simply-connected, e.g. without any "hole". 
    NOTE: User-specific modifications may be needed regarding the spatial diemnsions and coordinates of 
    the computational mesh. 

    Args: 
       `info`: dict
          * `dataPath`: string, path at which Nek5000 `<caseName>0.f*` files are stored.
          * `caseName`: string, Nek5000 case name <caseName>.
          * `startID`: int, starting number of `<caseName>0.f*` to be included in the database.
          * `endID`: int, ending number of `<caseName>0.f*` to be included in the database.
       `qoiName`: string, name of the quantity of interest  whose Nek5000 data to be read. 
          The valid QoIs include, 'ux', 'uy', 'uz', 'temperature, 'pressure'. Note that the exact list 
          depends on the Nek5000 case. 

    Returns:
       `db`: dict, database of the data of QoU name `qoiName` with the following keys:
          * 'snapDB': numpy array of shape (nx,ny,nt) containing the snapshot data of Nek5000
          * 'qoiName': string, name of QoI (determined by Nek5000)
          * 'nSnap': int, number of snapshots included in `db`          
          * 'dims': list of active spatial dimensions of the data, includes two or the three of 'x','y','z'.
          * 'x', 'y', 'z': numpy arrays containing coordinates of the GLL points in the Nek5000 mesh.
          * 'nx', 'ny', 'nz': int, number of grid points in each spatial direction.
          * 'creattionData': string, data of creation of `db`
          * 'startFile': int, the starting id of `<caseName>0.f*` included in the 'db'
          * 'endFile': int, the ending id of `<caseName>0.f*` included in the 'db'
    """
    path_=info['dataPath']
    caseName_=info['caseName']
    start_=info['startID']
    end_=info['endID']
    
    snapDB=[]
    pre_=caseName_+'0.f'
    nSnap=0
    for id_ in range(start_,end_+1):
        nSnap+=1
        db=dataset.open_dataset(path_+pre_+str(id_).zfill(5))

        if id_==start_:   #assuming mesh does not change with time
           nx=db['x'].shape[0]
           ny=db['y'].shape[0]
           nz=db['z'].shape[0]
           if nz==1:
              x=db['xmesh'][0,:,:]
              y=db['ymesh'][0,:,:]
           elif nz>1:   
              z=db['zmesh'][0,:,:]

        if nz==1:      
           qoi=db[qoiName][0,:,:]   #remove z
        snapDB.append(qoi)

    snapDB=np.asarray(snapDB)
    if nz==1:
       snapDB=np.transpose(snapDB,axes=[2,1,0])   #(nx,ny,nSnap), nx=Ex*nGLL, ny=Ey*nGLL
    elif nz>1:
       snapDB=np.transpose(snapDB,axes=[3,2,1,0])   #(nx,ny,nz,nSnap), nx=Ex*nGLL, ny=Ey*nGLL, nz=Ez*nGLL

    time_=datetime.now()
    time_= time_.strftime("%d/%m/%Y %H:%M:%S")
    db={'snapDB':snapDB,
        'qoiName':qoiName,    
        'nSnap':nSnap,    
        'dims':['x','y'],  #x, y: default active dims
        'x':x,
        'y':y,
        'nx':nx,
        'ny':ny,
        'creationDate':time_,
        'startFile':pre_+str(start_).zfill(5),
        'endFile':pre_+str(end_+1).zfill(5)
        }
    if nz>1:
        db.update({'nz':nz,'z':z,'dims':db['dims'].append('z')}) 
    return db    
#
def massMatrixReader(info):
    """
    Reading the mass matrix of a Nek5000 simulation
    Note: 
       * Name of the file containing the mass matrix data: `"MAS"+caseName+'0.f00001'`
       * The first entry in the file is supposed to contain the mass matrix. 
          Therefore, in the `*.usr` file of Nek5000 case one needs to have the following 
          in the `userchck`:
            `if (ISTEP.eq.0 .and. nid.eq.0) then
                call outpost(bm1,ym1,zm1,bm1,t,'MAS')
             endif`

    Args: 
       `info`: dict
          * `dataPath`: string, path at which Nek5000 `<caseName>0.f*` files are stored.
          * `caseName`: string, Nek5000 case name <caseName>.
          * `startID`: int, starting number of `<caseName>0.f*` to be included in the database.
          * `endID`: int, ending number of `<caseName>0.f*` to be included in the database.
       `qoiName`: string, name of the quantity of interest  whose Nek5000 data to be read. 
          The valid QoIs include, 'ux', 'uy', 'uz', 'temperature, 'pressure'. Note that the exact list 
          depends on the Nek5000 case. 

    Returns:
       `db`: dict, database of the data of QoI `qoiName` with the following keys:
          * 'snapDB': numpy array of shape (nx,ny,(nz),nt) containing the snapshot data of Nek5000
          * 'qoiName': string, name of QoI (determined by Nek5000)
          * 'nSnap': int, number of snapshots included in `db`          
          * 'dims': list of active spatial dimensions of the data, includes two or the three of 'x','y'(,'z'). 
          * 'x', 'y', 'z': numpy arrays containing coordinates of the GLL points in the Nek5000 mesh.
          * 'nx', 'ny', 'nz': int, number of grid points in each spatial direction.
          * 'creationData': string, data of creation of `db`.
          * 'startFile': int, the starting id of `<caseName>0.f*` included in the 'db'
          * 'endFile': int, the ending id of `<caseName>0.f*` included in the 'db'
    """
    path_=info['dataPath']
    caseName_=info['caseName']
    fileName='MAS'+caseName_+'0.f00001'
    db_=dataset.open_dataset(path_+fileName)

    #HERE
    bm1=db_['ux'][0,:,:].T   #assume bm1 is written at 1st entry when outposting in Nek5000
    bm1=np.asarray(bm1)

    nx=db_['x'].shape[0]
    ny=db_['y'].shape[0]
    nz=db_['z'].shape[0]
    if nz==1:
       x=db_['xmesh'][0,:,:]
       y=db_['ymesh'][0,:,:]
    elif nz>1:   
       z=db_['zmesh'][0,:,:]

    dbm={'val':bm1,
        'qoiName':'massMatrix',    
        'dims':['x','y'],
        'x':x,
        'y':y,
        'nx':nx,
        'ny':ny,
        'creationDate':datetime.now(),
        }
    return dbm
#
def dbReader(info):
    """
    Read pickled snapshot data created by `dbCreator(...)`
    """
    pickleFile_=info['pickleFile']
    with open(pickleFile_,'rb') as F:
         db=pickle.load(F)
    return db     
#         
